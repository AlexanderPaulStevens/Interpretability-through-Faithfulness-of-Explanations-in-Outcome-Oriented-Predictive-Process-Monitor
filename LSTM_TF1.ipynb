{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Y3PAY3qbT4x"
   },
   "source": [
    "# Analyzing the Spectrum of Explainability in Outcome-Oriented Predictive Process Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nfPTuq9bT4y"
   },
   "source": [
    "## traffic_fines_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cug_bCFubT4y"
   },
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJD4mKi1bT4y"
   },
   "source": [
    "#### importing libraries and terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5IiUd7lbT4y"
   },
   "outputs": [],
   "source": [
    "#import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQDp4i8Tm61-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZHxcgUJtEBm"
   },
   "outputs": [],
   "source": [
    "#user specified packages from Teinemaa\n",
    "from DatasetManager import DatasetManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBMyjQ5Vnn17"
   },
   "outputs": [],
   "source": [
    "#tree packages\n",
    "import graphviz\n",
    "from sklearn import tree\n",
    "\n",
    "#LSTM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, Input, Reshape, Activation, Multiply, TimeDistributed, concatenate, BatchNormalization, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxzIiPCxsxaj"
   },
   "source": [
    "Terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGpBNe_xm62A"
   },
   "outputs": [],
   "source": [
    "params_dir = './params_dir'\n",
    "results_dir = './results' \n",
    "cls_encoding = 'agg'\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "bucket_encoding = \"agg\"\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(1,6)],\n",
    "    \"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "}\n",
    "\n",
    "encoding_dict = {\n",
    "    \"laststate\": [\"static\", \"last\"],\n",
    "    \"agg\": [\"static\", \"agg\"],\n",
    "    \"index\": [\"static\", \"index\"],\n",
    "    \"combined\": [\"static\", \"last\", \"agg\"],\n",
    "    \"onehot\": [\"static\", \"onehot\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIh5L8EQm62B"
   },
   "outputs": [],
   "source": [
    "#terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8y0UarLm62B"
   },
   "outputs": [],
   "source": [
    "dataset_ref = 'traffic_fines_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqNZ9a0Qm62B"
   },
   "outputs": [],
   "source": [
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "methods = encoding_dict[cls_encoding]\n",
    "    \n",
    "train_ratio = 0.8\n",
    "random_state = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6W218XZjm62C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtmVGYqzm62C"
   },
   "outputs": [],
   "source": [
    "# print dataset name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFg7YhYxm62C",
    "outputId": "e9023251-53f0-4038-ead6-1e65bb96b7b7"
   },
   "outputs": [],
   "source": [
    "for dataset_name in datasets:\n",
    "    print('Dataset:', dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ls-Lee7pm62D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4iSr4inm62D"
   },
   "outputs": [],
   "source": [
    "# read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHiDYGSLuwm-"
   },
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "vvYE2Ig2m62D",
    "outputId": "5f7919c1-9dd4-44e1-c3c7-d695cd6e08c4"
   },
   "outputs": [],
   "source": [
    "data = dataset_manager.read_dataset()\n",
    "cls_encoder_args = {'case_id_col': dataset_manager.case_id_col, \n",
    "                        'static_cat_cols': dataset_manager.static_cat_cols,\n",
    "                        'static_num_cols': dataset_manager.static_num_cols, \n",
    "                        'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n",
    "                        'dynamic_num_cols': dataset_manager.dynamic_num_cols, \n",
    "                        'fillna': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2oAVuW-bT40"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpE1uKqxbT40"
   },
   "outputs": [],
   "source": [
    "#terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2ay4PnXbT40"
   },
   "outputs": [],
   "source": [
    "params_dir = './params_dir'\n",
    "results_dir = './results' \n",
    "DT_dir = './DT_dir'\n",
    "cls_encoding = 'agg'\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(1,6)],\n",
    "    \"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "}\n",
    "\n",
    "encoding_dict = {\n",
    "    \"laststate\": [\"static\", \"last\"],\n",
    "    \"agg\": [\"static\", \"agg\"],\n",
    "    \"index\": [\"static\", \"index\"],\n",
    "    \"combined\": [\"static\", \"last\", \"agg\"],\n",
    "    \"onehot\": [\"static\", \"onehot\"]\n",
    "}\n",
    "\n",
    "train_ratio = 0.8\n",
    "random_state = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHSFGVSVbT41"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OL-Uy69UbT44"
   },
   "outputs": [],
   "source": [
    "# determine min and max (truncated) prefix lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ebg7rz1dbT44"
   },
   "outputs": [],
   "source": [
    "min_prefix_length = 1\n",
    "if \"traffic_fines\" in dataset_name:\n",
    "    max_prefix_length = 10\n",
    "elif \"bpic2017\" in dataset_name:\n",
    "    max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "else:\n",
    "    max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFpiBH4IbT45"
   },
   "outputs": [],
   "source": [
    "max_prefix_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1ule9jQbT4-"
   },
   "source": [
    "#### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoaTLjvGbT4_"
   },
   "outputs": [],
   "source": [
    "# split into training and test\n",
    "train, test = dataset_manager.split_data_strict(data, train_ratio, split=\"temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_XIVPrBbT4_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5MSiJ4ubT4_"
   },
   "outputs": [],
   "source": [
    "#prefix generation of train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "um56lzYwbT4_",
    "outputId": "42bb86ed-2c3c-46ff-aef6-9e34ff1bfaa5"
   },
   "outputs": [],
   "source": [
    "print('prefixing started')\n",
    "dt_train_prefixes = dataset_manager.generate_prefix_data(train, min_prefix_length, max_prefix_length)\n",
    "dt_test_prefixes = dataset_manager.generate_prefix_data(test, min_prefix_length, max_prefix_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XA08ShvbT5A"
   },
   "outputs": [],
   "source": [
    "dt_train_prefixes_original = dt_train_prefixes.copy()\n",
    "dt_test_prefixes_original = dt_test_prefixes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffvhh9dybT5A"
   },
   "outputs": [],
   "source": [
    "#get the label of the train and test set\n",
    "test_y = dataset_manager.get_label_numeric(dt_test_prefixes)\n",
    "train_y = dataset_manager.get_label_numeric(dt_train_prefixes)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7455vq80bT5A"
   },
   "outputs": [],
   "source": [
    "auc_totals = []\n",
    "preds_all = []\n",
    "test_y_all = []\n",
    "nr_events_all = []\n",
    "offline_total_times = []\n",
    "online_event_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_099Nah8bT5B"
   },
   "outputs": [],
   "source": [
    "nr_events_all.extend(list(dataset_manager.get_prefix_lengths(dt_test_prefixes)))  \n",
    "test_y_all.extend(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJzBGCe7bT5s"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSDOMFk0bT6q"
   },
   "source": [
    "#### poging 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40mw3Q1Aq0eO"
   },
   "outputs": [],
   "source": [
    "#functions\n",
    "\n",
    "def prepare_inputs(X_train, X_test, data):    \n",
    "    oe = OrdinalEncoder()\n",
    "    oe.fit(data)\n",
    "    X_train_enc = oe.transform(X_train)\n",
    "    X_test_enc = oe.transform(X_test)\n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "def numeric_padding(sequences, maxlen=None, value=0):\n",
    "    num_samples = len(sequences)\n",
    "    sample_shape = np.asarray(sequences[0]).shape[1:]\n",
    "    x = np.full((num_samples, maxlen) + sample_shape, value)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        trunc = s[:maxlen]\n",
    "        x[idx, :maxlen] = trunc[0]\n",
    "        \n",
    "def normalist(lst):\n",
    "    s = sum(lst)\n",
    "    norm = [float(i)/s for i in lst]\n",
    "    return norm\n",
    "\n",
    "    return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFORMATION\n",
    "\n",
    "#categorical columns integercoded\n",
    "\n",
    "dt_train_prefixes.dtypes\n",
    "\n",
    "cat_columns = ['Activity']\n",
    "\n",
    "dt_train_prefixes[cat_columns],dt_test_prefixes[cat_columns]= prepare_inputs(dt_train_prefixes[cat_columns], dt_test_prefixes[cat_columns], data[cat_columns])\n",
    "\n",
    "\n",
    "\n",
    "dt_train_prefixes[cat_columns] = dt_train_prefixes[cat_columns]+1\n",
    "\n",
    "dt_test_prefixes[cat_columns] = dt_test_prefixes[cat_columns]+1\n",
    "\n",
    "#make embeddings\n",
    "\n",
    "dt_train_prefixes.dtypes\n",
    "\n",
    "cat_columns.append('Case ID')\n",
    "cat_columns.append('label')\n",
    "\n",
    "cat_columns\n",
    "\n",
    "ans_train = [pd.DataFrame(y) for x, y in dt_train_prefixes[cat_columns].groupby('Case ID', as_index=False)]\n",
    "ans_test = [pd.DataFrame(y) for x, y in dt_test_prefixes[cat_columns].groupby('Case ID', as_index=False)]\n",
    "\n",
    "train_labels = []\n",
    "for i in range (0,len(ans_train)):\n",
    "    temp_label = ans_train[i]['label'].iloc[0]\n",
    "    train_labels.append(temp_label)\n",
    "\n",
    "test_labels = []\n",
    "for i in range (0,len(ans_test)):\n",
    "    temp_label = ans_test[i]['label'].iloc[0]\n",
    "    test_labels.append(temp_label)\n",
    "\n",
    "train_y = [1 if i!='regular' else 0 for i in train_labels]\n",
    "test_y = [1 if i!='regular' else 0 for i in test_labels]\n",
    "\n",
    "cat_columns.remove('label')\n",
    "cat_columns.remove('Case ID')\n",
    "\n",
    "cat_columns\n",
    "\n",
    "maxlen = max_prefix_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutoff: max amount of events in a prefix\n",
    "cutoff = maxlen\n",
    "no_activities= len(data.groupby(['Activity']))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding of the different categorical columns\n",
    "#train paddings\n",
    "paddings_train = []\n",
    "for i in cat_columns:\n",
    "    padding= []\n",
    "    for k in range(0,len(ans_train)):\n",
    "        temp = []\n",
    "        temp = list(ans_train[k][i])\n",
    "        padding.append(temp)\n",
    "    \n",
    "    padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n",
    "    padded = padded/len(data.groupby([i]))\n",
    "    paddings_train.append(padded)\n",
    "\n",
    "#test paddings\n",
    "paddings_test = []\n",
    "for i in cat_columns:\n",
    "    padding= []\n",
    "    for k in range(0,len(ans_test)):\n",
    "        temp = []\n",
    "        temp = list(ans_test[k][i])\n",
    "        padding.append(temp)\n",
    "    \n",
    "    padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n",
    "    padded = padded/len(data.groupby([i]))\n",
    "    paddings_test.append(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### numerical columns\n",
    "#NUMERICAL COLUMNS\n",
    "numerical_columns = ['timesincelastevent',\n",
    "'Case ID']\n",
    "\n",
    "dt_train_prefixes2 = dt_train_prefixes[numerical_columns]\n",
    "dt_test_prefixes2 = dt_test_prefixes[numerical_columns]\n",
    "\n",
    "ans_train2 = [pd.DataFrame(y) for x,y in dt_train_prefixes2.groupby('Case ID', as_index=False)]\n",
    "ans_test2 = [pd.DataFrame(y) for x,y in dt_test_prefixes2.groupby('Case ID', as_index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_train = []\n",
    "pad_test  = []\n",
    "num_columns = ['timesincelastevent']\n",
    "for i in num_columns:\n",
    "    padding = []\n",
    "    for k in range(0,len(ans_train2)):\n",
    "        temp_train = []\n",
    "        temp_train = list(ans_train2[k][i])\n",
    "        padding.append(temp_train)\n",
    "        \n",
    "    padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n",
    "    padded = padded/data[i].max()\n",
    "    pad_train.append(padded)\n",
    "for i in num_columns:\n",
    "    padding = []\n",
    "    for k in range(0,len(ans_test2)):\n",
    "        temp_test = []\n",
    "        temp_test = list(ans_test2[k][i])\n",
    "        padding.append(temp_test)\n",
    "        \n",
    "    padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n",
    "    padded = padded/data[i].max()\n",
    "    pad_test.append(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OjrU_tubT65"
   },
   "source": [
    "#### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guya9W9RrQEi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.constraints import non_neg, Constraint\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import Nadam, Adam, SGD, Adagrad\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Bidirectional,TimeDistributed\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Softmax, Lambda, Multiply, Dropout\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "import tensorflow.keras.utils as ku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VG__EYy9rQ9C"
   },
   "outputs": [],
   "source": [
    "def create_index(log_df, column):\n",
    "    \"\"\"Creates an idx for a categorical attribute.\n",
    "    Args:\n",
    "        log_df: dataframe.\n",
    "        column: column name.\n",
    "    Returns:\n",
    "        index of a categorical attribute pairs.\n",
    "    \"\"\"\n",
    "    temp_list = log_df[[column]].values.tolist()\n",
    "    subsec_set = {(x[0]) for x in temp_list}\n",
    "    subsec_set = sorted(list(subsec_set))\n",
    "    alias = dict()\n",
    "    for i, _ in enumerate(subsec_set):\n",
    "        alias[subsec_set[i]] = i + 1\n",
    "    return alias\n",
    "\n",
    "\n",
    "ac_index = create_index(dt_train_prefixes_original, 'Activity')\n",
    "ac_index['Start'] = 0\n",
    "ac_index['End'] = len(ac_index)\n",
    "index_ac = {v: k for k, v in ac_index.items()}\n",
    "\n",
    "ac_weights = ku.to_categorical(sorted(index_ac.keys()), len(ac_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4S2LPBqrQ_i"
   },
   "outputs": [],
   "source": [
    "dropout_input = 0.01\n",
    "dropout_context=0.30\n",
    "lstm_size_alpha=64\n",
    "lstm_size_beta=64\n",
    "print(\"Training prefix and variable attention model\")\n",
    "\n",
    "l2reg=0.001\n",
    "allow_negative=False\n",
    "incl_time = True \n",
    "incl_res = True\n",
    "#Code Input\n",
    "#categorical embeddings\n",
    "input_layer_a = Input(shape=(cutoff,), name='activity_input')\n",
    "\n",
    "time_input_layer = Input(shape=(cutoff,1), name='time_input')\n",
    "\n",
    "########################################\n",
    "#inputs_list = [ac_input]\n",
    "   \n",
    "#Calculate embedding for each code and sum them to a visit level\n",
    "embedding_activity = Embedding(ac_weights.shape[0],\n",
    "                            ac_weights.shape[1],\n",
    "                            weights=[ac_weights],\n",
    "                            input_length=no_activities,\n",
    "                           name='ac_embedding')(input_layer_a)\n",
    "dim =ac_weights.shape[1]\n",
    "\n",
    "\n",
    "full_embs = embedding_activity\n",
    "\n",
    "#Apply dropout on inputs\n",
    "full_embs = Dropout(dropout_input)(full_embs)\n",
    "time_embs = concatenate([full_embs, time_input_layer], name='allInp')\n",
    "\n",
    "dim += 1\n",
    "\n",
    "alpha = Bidirectional(LSTM(lstm_size_alpha, return_sequences=True),\n",
    "                                    name='alpha')\n",
    "beta = Bidirectional(LSTM(lstm_size_beta, return_sequences=True),\n",
    "                                   name='beta')\n",
    "alpha_dense = Dense(1, kernel_regularizer=l2(l2reg))\n",
    "beta_dense = Dense(dim, activation='tanh', kernel_regularizer=l2(l2reg))\n",
    "\n",
    "#Compute alpha, visit attention\n",
    "alpha_out = alpha(time_embs)\n",
    "alpha_out = TimeDistributed(alpha_dense, name='alpha_dense_0')(alpha_out)\n",
    "alpha_out = Softmax(axis=1, name='alpha_softmax')(alpha_out)\n",
    "#Compute beta, codes attention\n",
    "beta_out = beta(time_embs)\n",
    "beta_out = TimeDistributed(beta_dense, name='beta_dense_0')(beta_out)\n",
    "#Compute context vector based on attentions and embeddings\n",
    "c_t = Multiply()([alpha_out, beta_out, time_embs])\n",
    "c_t = Lambda(lambda x: backend.sum(x, axis=1))(c_t)\n",
    "        #Reshape to 3d vector for consistency between Many to Many and Many to One implementations\n",
    "        #contexts = L.Lambda(reshape)(c_t)\n",
    "\n",
    "#Make a prediction\n",
    "contexts = Dropout(dropout_context)(c_t)\n",
    " \n",
    "output_layer = Dense(1, activation='sigmoid', name='final_output')(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wF14AingrRFC"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_layer_a, time_input_layer], outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36ANnG1lrRHi"
   },
   "outputs": [],
   "source": [
    "opt = Nadam(lr=0.0005, beta_1=0.9, beta_2=0.999,\n",
    "                   epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "\n",
    "model.compile(loss={'final_output':'binary_crossentropy'}, optimizer= opt)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = os.path.join('./', 'models/model_rd_'+'_{epoch:02d}-{val_loss:.2f}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMA9Kw72rb2r"
   },
   "outputs": [],
   "source": [
    "padded_time = np.reshape(pad_train[0], (len(pad_train[0]), cutoff, 1))\n",
    "padded_time_test=  np.reshape(pad_test[0], (len(pad_test[0]), cutoff, 1))\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(output_file_path,\n",
    "                                       monitor='val_loss',\n",
    "                                       verbose=1,\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=False,\n",
    "                                       mode='auto')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                   factor=0.5,\n",
    "                                   patience=2,\n",
    "                                   verbose=0,\n",
    "                                   mode='auto',\n",
    "                                   min_delta=0.0001,\n",
    "                                   cooldown=0,\n",
    "                                   min_lr=0)\n",
    "model_inputs = [paddings_train[0]]\n",
    "model_inputs.append(padded_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.fit(model_inputs,\n",
    "              train_y,\n",
    "              callbacks=[early_stopping, lr_reducer, model_checkpoint],\n",
    "              validation_split = 0.1,\n",
    "              verbose=1,\n",
    "              epochs=10)\n",
    "model_inputs_test = [paddings_test[0]]\n",
    "model_inputs_test.append(padded_time_test)\n",
    "predictions = model.predict(model_inputs_test)\n",
    "auc = roc_auc_score(test_y,predictions)\n",
    "total_time = time.time() - start\n",
    "print(total_time)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'BPIC2017_PC10'\n",
    "model_json = model.to_json()\n",
    "with open('./models/'+model_name+\".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights('./models/'+model_name+\".h5\")\n",
    "            print(\"Saved model to disk\")\n",
    "            print('Name: '+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_all = list(predictions)\n",
    "method_name = \"attention\"\n",
    "cls_method = \"LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gap > 1:\n",
    "    outfile = os.path.join(results_dir, \"performance_results_%s_%s_%s_gap%s.csv\" % (cls_method, dataset_name, method_name, gap))\n",
    "else:\n",
    "    outfile = os.path.join(results_dir, \"performance_results_%s_%s_%s.csv\" % (cls_method, dataset_name, method_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nr_events_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outfile, 'w') as fout:\n",
    "        fout.write(\"%s;%s;%s;%s;%s;%s\\n\"%(\"dataset\", \"method\", \"cls\", \"nr_events\", \"metric\", \"score\"))\n",
    "        fout.write(\"%s;%s;%s;%s;%s;%s\\n\"%(dataset_name, method_name, cls_method,-1, \"runtime_total\", total_time))\n",
    "        fout.write(\"%s;%s;%s;%s;%s;%s\\n\"%(dataset_name, method_name, cls_method,-1, \"runtime_avg\", total_time/10))  \n",
    "        dt_results = pd.DataFrame({\"actual\": test_y_all, \"predicted\": preds_all, \"nr_events\": nr_events_all})\n",
    "        for nr_events, group in dt_results.groupby(\"nr_events\"):\n",
    "            if len(set(group.actual)) < 2:\n",
    "                fout.write(\"%s;%s;%s;%s;%s;%s;%s\\n\"%(dataset_name, method_name, cls_method, nr_events,-1, \"auc\", np.nan))\n",
    "            else:\n",
    "                fout.write(\"%s;%s;%s;%s;%s;%s;%s\\n\"%(dataset_name, method_name, cls_method, nr_events,-1, \"auc\", roc_auc_score(group.actual, group.predicted)))\n",
    "        fout.write(\"%s;%s;%s;%s;%s;%s\\n\"%(dataset_name, method_name, cls_method,-1, \"auc\", roc_auc_score(dt_results.actual, dt_results.predicted)))\n",
    "        fout.write(\"%s\\n\"%(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Opening model')\n",
    "json_file = open('./models/'+model_name+'.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model2 = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "print('Opening weights')\n",
    "model2.load_weights('./models/'+model_name+\".h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Next event selection method and numbers of repetitions\n",
    "#variants = [{'imp': 'Arg Max', 'rep': 1}]#,\n",
    "               # {'imp': 'Random Choice', 'rep': 1}]\n",
    "#   Generation of predictions\n",
    "has_time=False\n",
    "    #model = load_model(os.path.join(output_route, parameters['model_file']))\n",
    "model = model\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "print(layer_names)\n",
    "l_emb_weights=None\n",
    "ac_emb_weights = model.get_layer(name='ac_embedding').get_weights()[0]\n",
    "\n",
    "s_emb_weights = model.get_layer(name='s_embedding').get_weights()[0]\n",
    "    \n",
    "at_emb_weights = model.get_layer(name='at_embedding').get_weights()[0]  \n",
    "    \n",
    "acc_emb_weights = model.get_layer(name='acc_embedding').get_weights()[0]  \n",
    "    \n",
    "li_emb_weights = model.get_layer(name='li_embedding').get_weights()[0]  \n",
    "\n",
    "has_time=True\n",
    "\n",
    "ac_output_weights, ac_bias = model.get_layer(name='final_output').get_weights()\n",
    "print(ac_output_weights)\n",
    "model_with_attention = Model(model.inputs, model.outputs +\\\n",
    "                                              [model.get_layer(name='alpha_softmax').output,\\\n",
    "                                               model.get_layer(name='beta_dense_0').output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_vectors = []\n",
    "variable_vectors=[]\n",
    "predictions = []\n",
    "for i in range(0,len(paddings_test[0])):\n",
    "    x_ac_ngram = paddings_test[0][i].reshape((1,cutoff))\n",
    "    x_s_ngram = paddings_test[1][i].reshape((1,cutoff))\n",
    "    x_at_ngram = paddings_test[2][i].reshape((1,cutoff))\n",
    "    x_acc_ngram = paddings_test[3][i].reshape((1,cutoff))\n",
    "    x_li_ngram = paddings_test[4][i].reshape((1,cutoff))\n",
    "    x_in_ngram  = pad_creditscore_test[i].reshape(1, 40, 1)\n",
    "    x_t_ngram   = padded_time_test[i].reshape(1, 40, 1)\n",
    "    betas=None      \n",
    "    proba, alphas, betas = model_with_attention.predict([x_ac_ngram, x_s_ngram, x_at_ngram, x_acc_ngram, x_li_ngram, x_in_ngram,x_t_ngram])\n",
    "    #print(proba, alphas, betas)\n",
    "    proba = np.squeeze(proba)\n",
    "    alphas = np.squeeze(alphas)\n",
    "    temporal_att_vec = alphas\n",
    "    assert (np.sum(temporal_att_vec) - 1.0) < 1e-5\n",
    "    #print(temporal_att_vec)\n",
    "    temporal_vectors.append(temporal_att_vec)\n",
    "\n",
    "    if betas is not None:\n",
    "        #get the beta value\n",
    "        betas = np.squeeze(betas)\n",
    "        idx = np.argmax(alphas)\n",
    "        #print(idx)\n",
    "        beta_val = betas[idx]\n",
    "        # get the activity and role for that idx\n",
    "        act_ip = int(x_ac_ngram[0][idx])\n",
    "        ac_emb = ac_emb_weights[act_ip]\n",
    "        dim=ac_emb.shape[0]\n",
    "        \n",
    "        rol_ip = int(x_s_ngram[0][idx])\n",
    "        s_emb = s_emb_weights[rol_ip]\n",
    "        dim += s_emb.shape[0]\n",
    "            \n",
    "       \n",
    "        rol_ip = int(x_at_ngram[0][idx])\n",
    "        at_emb = at_emb_weights[rol_ip]\n",
    "        dim += at_emb.shape[0]\n",
    "        \n",
    "        \n",
    "        rol_ip = int(x_acc_ngram[0][idx])\n",
    "        acc_emb = acc_emb_weights[rol_ip]\n",
    "        dim += acc_emb.shape[0]\n",
    "           \n",
    "        \n",
    "        rol_ip = int(x_li_ngram[0][idx])\n",
    "        li_emb = li_emb_weights[rol_ip]\n",
    "        dim += li_emb.shape[0]\n",
    "            \n",
    "        num_v = np.squeeze(x_in_ngram)[idx]\n",
    "        dim +=1\n",
    "        \n",
    "        if(betas.shape[1]==dim+1):\n",
    "            time_v = np.squeeze(x_t_ngram)[idx]  # time and role as masked together\n",
    "            emb = np.concatenate((ac_emb,s_emb,at_emb,acc_emb,li_emb,num_v,time_v), axis=None)\n",
    "          \n",
    "        #print('beta_val',beta_val.shape)\n",
    "        beta_scaled = np.multiply(beta_val,emb)\n",
    "        variable_attn = alphas[idx] * beta_scaled\n",
    "        #sum_grad = np.sum(ac_output_weights, axis=1)\n",
    "        #variable_attn=np.multiply(sum_grad.flatten(), variable_attn)\n",
    "        #if pos!=test_y[i]:\n",
    "            #temporal_vectors[i] = np.zeros(40, dtype='float32')\n",
    "            #variable_attn = np.zeros(52, dtype='float32')\n",
    "            #variable_vectors.append(variable_attn)\n",
    "        predictions.append(proba)\n",
    "        variable_vectors.append(variable_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(test_y, predictions)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_final = np.mean(np.array(temporal_vectors), axis=0)\n",
    "pd.DataFrame(temp_final, columns=['alpha attention weight']).plot(kind='bar',\n",
    "                                                                        title='Attention of '\n",
    "                                                                              ' index', figsize=(10,7))\n",
    "# Hide grid lines\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(variable_vectors)>0):\n",
    "    var_final = np.mean(np.array(variable_vectors), axis=0)\n",
    "    ac_labels = [index_ac[key] for key in sorted(index_ac.keys())]\n",
    "    s_labels = [index_s[key] for key in sorted(index_s.keys())]\n",
    "    at_labels = [index_at[key] for key in sorted(index_at.keys())]\n",
    "    acc_labels = [index_acc[key] for key in sorted(index_acc.keys())]\n",
    "    li_labels = [index_li[key] for key in sorted(index_li.keys())]\n",
    "    print(ac_labels)\n",
    "    print(s_labels)\n",
    "    print(at_labels)\n",
    "    print(acc_labels)\n",
    "    print(li_labels)\n",
    "      \n",
    "    num_dim = var_final.shape[0]\n",
    "    print(num_dim)\n",
    "      \n",
    "    if s_emb_weights is not None:\n",
    "        ac_labels.extend(s_labels)\n",
    "    if at_emb_weights is not None:\n",
    "        ac_labels.extend(at_labels)  \n",
    "    if acc_emb_weights is not None:\n",
    "        ac_labels.extend(acc_labels)\n",
    "    if li_emb_weights is not None:\n",
    "        ac_labels.extend(li_labels)\n",
    "    ac_labels.append('CreditScore')\n",
    "    ac_labels.append('time')\n",
    "        \n",
    "    df_var=pd.DataFrame({'attributes':var_final, 'attribute_values':ac_labels})\n",
    "    print(df_var)\n",
    "    df_var.plot.bar(y='attributes', x='attribute_values',\n",
    "                                title='Attention of the event attributes.', figsize=(10,7))\n",
    "                                                                               \n",
    "    #plot_history( plt, file_name + 'variable_attn', path )\n",
    "      \n",
    "\n",
    "    # Hide grid lines\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_local = variable_vectors[200]\n",
    "df_var_local=pd.DataFrame({'attributes':var_local, 'attribute_values':ac_labels})\n",
    "df_var_local.plot.bar(y='attributes', x='attribute_values',\n",
    "                          title='Attention of the event attributes.')\n",
    "                                                                               \n",
    "plt.show()\n",
    "#plotting local explanation\n",
    "local_var = temporal_vectors[19]\n",
    "pd.DataFrame(local_var, columns=['alpha attention weight']).plot(kind='bar',\n",
    "                                                                        title='Attention of '\n",
    "                                                                               ' index')                                                             \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Kopie van Leafmodel_BPIC2017_accepted.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "590.66px",
    "left": "34.9931px",
    "top": "110.926px",
    "width": "164.988px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
