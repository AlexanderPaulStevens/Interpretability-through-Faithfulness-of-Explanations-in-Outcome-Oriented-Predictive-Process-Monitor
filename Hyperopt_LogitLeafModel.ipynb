{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt parameters leafmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Feature Selection BPIC2015 2 f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#12 columns (20% correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns_PC20= ['agg__Activity_08_AWB45_020_2',\n",
    " 'static__Milieu (vergunning)',\n",
    " 'agg__question_28',\n",
    " 'agg__monitoringResource_4634935',\n",
    " 'agg__Activity_08_AWB45_025',\n",
    " 'agg__question_Uitgebreid',\n",
    " 'agg__Activity_08_AWB45_170',\n",
    " 'agg__Activity_01_HOOFD_330',\n",
    " 'agg__org:resource_560530',\n",
    " 'agg__org:resource_4634935',\n",
    " 'agg__Activity_01_HOOFD_193',\n",
    " 'agg__question_other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#22 columns (15% correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns_PC15= ['agg__Activity_08_AWB45_020_2',\n",
    " 'static__Milieu (vergunning)',\n",
    " 'agg__question_28',\n",
    " 'agg__monitoringResource_4634935',\n",
    " 'agg__Activity_08_AWB45_025',\n",
    " 'agg__question_Uitgebreid',\n",
    " 'agg__Activity_08_AWB45_170',\n",
    " 'agg__Activity_01_HOOFD_330',\n",
    " 'agg__org:resource_560530',\n",
    " 'agg__org:resource_4634935',\n",
    " 'agg__Activity_01_HOOFD_193',\n",
    " 'agg__question_other',\n",
    " 'agg__Activity_08_AWB45_060',\n",
    " 'agg__Activity_01_HOOFD_191',\n",
    " 'agg__Activity_08_AWB45_051_0',\n",
    " 'agg__timesincelastevent_sum',\n",
    " 'agg__Activity_01_HOOFD_490_1',\n",
    " 'agg__Activity_01_HOOFD_250',\n",
    " 'agg__Activity_08_AWB45_090_2',\n",
    " 'static__Kap',\n",
    " 'static__Milieu (neutraal wijziging)',\n",
    " 'agg__Activity_01_HOOFD_490_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#50 columns (10% correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns_PC10= ['agg__Activity_08_AWB45_020_2',\n",
    " 'static__Milieu (vergunning)',\n",
    " 'agg__question_28',\n",
    " 'agg__monitoringResource_4634935',\n",
    " 'agg__Activity_08_AWB45_025',\n",
    " 'agg__question_Uitgebreid',\n",
    " 'agg__Activity_08_AWB45_170',\n",
    " 'agg__Activity_01_HOOFD_330',\n",
    " 'agg__org:resource_560530',\n",
    " 'agg__org:resource_4634935',\n",
    " 'agg__Activity_01_HOOFD_193',\n",
    " 'agg__question_other',\n",
    " 'agg__Activity_08_AWB45_060',\n",
    " 'agg__Activity_01_HOOFD_191',\n",
    " 'agg__Activity_08_AWB45_051_0',\n",
    " 'agg__timesincelastevent_sum',\n",
    " 'agg__Activity_01_HOOFD_490_1',\n",
    " 'agg__Activity_01_HOOFD_250',\n",
    " 'agg__Activity_08_AWB45_090_2',\n",
    " 'static__Kap',\n",
    " 'static__Milieu (neutraal wijziging)',\n",
    " 'agg__Activity_08_AWB45_070_3',\n",
    " 'static__Gebiedsbescherming',\n",
    " 'agg__month_std',\n",
    " 'agg__timesincecasestart_std',\n",
    " 'agg__Activity_01_HOOFD_250_1',\n",
    " 'agg__question_42',\n",
    " 'agg__Activity_01_HOOFD_130',\n",
    " 'agg__Activity_01_HOOFD_250_2',\n",
    " 'agg__weekday_std',\n",
    " 'agg__Activity_08_OLO_100',\n",
    " 'agg__Activity_01_HOOFD_190_1',\n",
    " 'static__Responsible_actor_560530',\n",
    " 'agg__Activity_11_AH_II_010',\n",
    " 'agg__Activity_08_AWB45_050',\n",
    " 'agg__timesincecasestart_mean',\n",
    " 'agg__weekday_max',\n",
    " 'agg__timesincelastevent_max',\n",
    " 'agg__Activity_08_AWB45_090_1',\n",
    " 'agg__timesincelastevent_std',\n",
    " 'agg__Activity_08_AWB45_020_0',\n",
    " 'agg__timesincecasestart_sum',\n",
    " 'agg__open_cases_std',\n",
    " 'agg__timesincelastevent_mean',\n",
    " 'agg__Activity_01_HOOFD_190_2',\n",
    " 'static__Inrit/Uitweg',\n",
    " 'agg__Activity_01_HOOFD_470',\n",
    " 'agg__Activity_01_HOOFD_510_2',\n",
    " 'agg__month_min',\n",
    " 'agg__Activity_01_HOOFD_495']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Feature Selection Traffic_Fines_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#15% PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns_PC15 = ['agg__Activity_Payment', 'agg__Activity_Send Fine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#10% PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns_PC10 = ['agg__Activity_Payment', 'agg__Activity_Send Fine', 'agg__Activity_Insert Date Appeal to Prefecture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Feature Selection BPIC2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#20% PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns_PC20= ['agg__CreditScore_std', 'agg__Selected_False']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#15% PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns_PC15= ['agg__CreditScore_std',\n",
    " 'agg__Selected_False',\n",
    " 'static__ApplicationType_New credit',\n",
    " 'static__ApplicationType_Limit raise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#10% PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns_PC10 = ['static__ApplicationType_Limit raise',\n",
    " 'static__ApplicationType_New credit',\n",
    " 'agg__Activity_A_Cancelled',\n",
    " 'agg__Activity_A_Submitted',\n",
    " 'agg__Activity_W_Validate application',\n",
    " 'agg__lifecycle:transition_ate_abort',\n",
    " 'agg__Accepted_False',\n",
    " 'agg__Accepted_True',\n",
    " 'agg__Selected_False',\n",
    " 'agg__CreditScore_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 5/5 [01:24<00:00, 16.96s/trial, best loss: -0.9508083448421693]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pickle\n",
    "\n",
    "import hyperopt\n",
    "from hyperopt import hp, Trials, fmin, tpe, STATUS_OK\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "#user-specified packages from Benchmark for outcome-oriented predictive process monitoring (see acknowledgements)\n",
    "import EncoderFactory\n",
    "from DatasetManager import DatasetManager\n",
    "\n",
    "def create_and_evaluate_model(args): \n",
    "    global trial_nr\n",
    "    trial_nr += 1\n",
    "    \n",
    "    start = time.time()\n",
    "    score = 0\n",
    "    for cv_iter in range(n_splits):\n",
    "        \n",
    "        dt_test_prefixes = dt_prefixes[cv_iter]\n",
    "        dt_train_prefixes = pd.DataFrame()\n",
    "        for cv_train_iter in range(n_splits): \n",
    "            if cv_train_iter != cv_iter:\n",
    "                dt_train_prefixes = pd.concat([dt_train_prefixes, dt_prefixes[cv_train_iter]], axis=0)\n",
    "        \n",
    "        #remove training rows with negative values (should be a mistake)\n",
    "        #dt_train_prefixes = dt_train_prefixes[dt_train_prefixes.select_dtypes(include=[np.number]).ge(0).all(1)]\n",
    "\n",
    "        preds_all = []\n",
    "        test_y_all = []\n",
    "        test_y = dataset_manager.get_label_numeric(dt_test_prefixes)\n",
    "        train_y = dataset_manager.get_label_numeric(dt_train_prefixes)  \n",
    "                 \n",
    "        #feature combiner\n",
    "        feature_combiner = FeatureUnion([(method, EncoderFactory.get_encoder(method, **cls_encoder_args)) for method in methods])\n",
    "        feature_combiner.fit(dt_train_prefixes, train_y)\n",
    "\n",
    "        #transform train dataset\n",
    "        dt_train_named= feature_combiner.transform(dt_train_prefixes)\n",
    "        dt_train_named = pd.DataFrame(dt_train_named)\n",
    "        names= feature_combiner.get_feature_names()\n",
    "        dt_train_named.columns = names\n",
    "        dt_train_named = dt_train_named[columns]\n",
    "        \n",
    "        \n",
    "        #first, create,train and fit a DecisionTreeClassifier\n",
    "        cls = DecisionTreeClassifier(criterion= 'entropy', max_depth= args['max_depth'], min_samples_leaf= args['min_samples_leaf'], random_state = random_state)\n",
    "        cls.fit(dt_train_named,train_y)\n",
    "       \n",
    "        #transform test dataset\n",
    "        dt_test_named = feature_combiner.transform(dt_test_prefixes)\n",
    "        dt_test_named = pd.DataFrame(dt_test_named)\n",
    "        names= feature_combiner.get_feature_names()\n",
    "        dt_test_named.columns = names\n",
    "        \n",
    "        \n",
    "        dt_test_named = dt_test_named[columns]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #homogeneuous segments (buckets)\n",
    "        dt_train_named['cluster'] = cls.apply(dt_train_named)\n",
    "        dt_test_named['cluster'] = cls.apply(dt_test_named)\n",
    "\n",
    "        #temporarily concatenate back, in order to have the correct y-values per segment\n",
    "        #train data \n",
    "        train_y_concat = pd.DataFrame(train_y)\n",
    "        train_y_concat = train_y_concat.rename(columns={train_y_concat.columns[0]:'label'})\n",
    "        dt_train_named = pd.concat([dt_train_named,train_y_concat], axis=1)\n",
    "        \n",
    "        #test data\n",
    "        test_y_concat  = pd.DataFrame(test_y)\n",
    "        test_y_concat  = test_y_concat.rename(columns={test_y_concat.columns[0]:'label'})\n",
    "        dt_test_named = pd.concat([dt_test_named, test_y_concat], axis=1)\n",
    "    \n",
    "        #list of buckets\n",
    "        buckets = list((dt_test_named['cluster'].unique()))\n",
    "        for i in buckets: \n",
    "            #only take the data from the bucket, seperate the label from the independent features\n",
    "            data_train_x = dt_train_named[dt_train_named['cluster']==i].copy()\n",
    "            data_train_y = data_train_x['label'].copy()\n",
    "    \n",
    "            data_test_x  = dt_test_named[dt_test_named['cluster']==i].copy()\n",
    "            data_test_y  = data_test_x['label'].copy()\n",
    "        \n",
    "            #drop the columns\n",
    "            data_train_x = data_train_x.drop('label', axis=1)\n",
    "            data_train_x = data_train_x.drop('cluster', axis=1)\n",
    "            data_test_x = data_test_x.drop('label', axis=1)\n",
    "            data_test_x = data_test_x.drop('cluster', axis=1)\n",
    "        \n",
    "            #if there is only one label in the training data, no need to create a leaf model\n",
    "            if len(set(data_train_y))<2:\n",
    "                pred = [data_train_y.iloc[0]]*len(data_test_y)\n",
    "                preds_all.extend(pred)\n",
    "                test_y_all.extend(data_test_y)\n",
    "                \n",
    "            else:  \n",
    "                #print length of test and training data of the leaf node\n",
    "                test_y_all.extend(data_test_y)\n",
    "            \n",
    "                if cls_method == 'llm':\n",
    "                    scaler = StandardScaler()\n",
    "                    data_train_x= scaler.fit_transform(data_train_x)\n",
    "                    data_test_x= scaler.transform(data_test_x)\n",
    "                    preds_pos_label_idx = np.where(cls.classes_ == 1)[0][0]\n",
    "                    logreg = LogisticRegression(max_iter= 10000, \n",
    "                                            solver='sag', \n",
    "                                            n_jobs =-1)\n",
    "                    logreg.fit(data_train_x, data_train_y)\n",
    "                    pred = logreg.predict_proba(data_test_x)\n",
    "                    pred = pred[:,preds_pos_label_idx]\n",
    "                    preds_all.extend(pred)   \n",
    "\n",
    "        score += roc_auc_score(test_y_all, preds_all)\n",
    "    \n",
    "    \n",
    "       \n",
    "        for k, v in args.items():\n",
    "            fout_all.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, k, v, score / n_splits))   \n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, \"processing_time\", time.time() - start, 0))   \n",
    "    fout_all.flush()\n",
    "    return {'loss': -score / n_splits, 'status': STATUS_OK, 'model': cls}\n",
    "\n",
    "dataset_ref = 'bpic2015_2_f2'\n",
    "params_dir = './params_dir'\n",
    "n_iter = 5\n",
    "cls_encoding = 'agg'\n",
    "cls_method = 'llm'\n",
    "\n",
    "column_selection = 'PC15'\n",
    "columns = columns_PC15\n",
    "\n",
    "method_name = \"%s_%s\"%(column_selection, cls_encoding)\n",
    "\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(1,6)]\n",
    "}\n",
    "\n",
    "encoding_dict = {\n",
    "    \"agg\": [\"static\", \"agg\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "methods = encoding_dict[cls_encoding]\n",
    "    \n",
    "train_ratio = 0.8\n",
    "n_splits = 3\n",
    "random_state = 22\n",
    "\n",
    "# create results directory\n",
    "if not os.path.exists(os.path.join(params_dir)):\n",
    "    os.makedirs(os.path.join(params_dir))\n",
    "    \n",
    "for dataset_name in datasets:\n",
    "    \n",
    "    # read the data\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "    data = dataset_manager.read_dataset()\n",
    "    cls_encoder_args = {'case_id_col': dataset_manager.case_id_col, \n",
    "                        'static_cat_cols': dataset_manager.static_cat_cols,\n",
    "                        'static_num_cols': dataset_manager.static_num_cols, \n",
    "                        'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n",
    "                        'dynamic_num_cols': dataset_manager.dynamic_num_cols, \n",
    "                        'fillna': True}\n",
    "\n",
    "    # determine min and max (truncated) prefix lengths\n",
    "    min_prefix_length = 1\n",
    "    if \"traffic_fines\" in dataset_name:\n",
    "        max_prefix_length = 10\n",
    "    elif \"bpic2017\" in dataset_name:\n",
    "        max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "    else:\n",
    "        max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "\n",
    "    # split into training and test\n",
    "    train, _ = dataset_manager.split_data_strict(data, train_ratio, split=\"temporal\")\n",
    "    \n",
    "    \n",
    "    # prepare chunks for CV\n",
    "    dt_prefixes = []\n",
    "    class_ratios = []\n",
    "    for train_chunk, test_chunk in dataset_manager.get_stratified_split_generator(train, n_splits=n_splits):\n",
    "        class_ratios.append(dataset_manager.get_class_ratio(train_chunk))\n",
    "        # generate data where each prefix is a separate instance\n",
    "        dt_prefixes.append(dataset_manager.generate_prefix_data(test_chunk, min_prefix_length, max_prefix_length))\n",
    "    del train\n",
    "        \n",
    "    # set up search space\n",
    "    \n",
    "    if cls_method == \"llm\":\n",
    "        space = {\"max_depth\": scope.int(hp.quniform(\"max_depth\",1,2,1)),\n",
    "                 \"min_samples_leaf\": scope.int(hp.quniform(\"min_samples_leaf\",100,1000,10))\n",
    "                }\n",
    "          \n",
    "    # optimize parameters\n",
    "    trial_nr = 0\n",
    "    trials = Trials()\n",
    "    fout_all = open(os.path.join(params_dir, \"param_optim_all_trials_%s_%s_%s.csv\" % (cls_method, dataset_name, method_name)), \"w\")\n",
    "    if \"prefix\" in method_name:\n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s;%s\\n\" % (\"iter\", \"dataset\", \"cls\", \"method\", \"nr_events\", \"param\", \"value\", \"score\"))   \n",
    "    else:\n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (\"iter\", \"dataset\", \"cls\", \"method\", \"param\", \"value\", \"score\"))   \n",
    "    rstate = np.random.RandomState(22)\n",
    "    best = fmin(create_and_evaluate_model, space, algo=tpe.suggest, max_evals=n_iter, trials=trials, rstate = rstate)\n",
    "    fout_all.close()\n",
    "\n",
    "    # write the best parameters\n",
    "    best_params = hyperopt.space_eval(space, best)\n",
    "    outfile = os.path.join(params_dir, \"optimal_params_%s_%s_%s.pickle\" % (cls_method, dataset_name, method_name))\n",
    "    # write to file\n",
    "    with open(outfile, \"wb\") as fout:\n",
    "        pickle.dump(best_params, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
