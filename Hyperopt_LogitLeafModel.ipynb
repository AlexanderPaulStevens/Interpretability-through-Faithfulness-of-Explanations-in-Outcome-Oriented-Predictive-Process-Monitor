{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt (Logit Leaf Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the use of this notebook, the optimal parameters for the Logit Leaf Model (LLM) are determined\n",
    "Note that this code is based on the code provided by https://github.com/irhete/predictive-monitoring-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset BPIC2015 2 f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out how these columns where determined, see the Feature Selection file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 columns (20% correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_PC20= ['agg__Activity_08_AWB45_020_2',\n",
    " 'static__Milieu (vergunning)',\n",
    " 'agg__question_28',\n",
    " 'agg__monitoringResource_4634935',\n",
    " 'agg__Activity_08_AWB45_025',\n",
    " 'agg__question_Uitgebreid',\n",
    " 'agg__Activity_08_AWB45_170',\n",
    " 'agg__Activity_01_HOOFD_330',\n",
    " 'agg__org:resource_560530',\n",
    " 'agg__org:resource_4634935',\n",
    " 'agg__Activity_01_HOOFD_193',\n",
    " 'agg__question_other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22 columns (15% correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_PC15= ['agg__Activity_08_AWB45_020_2',\n",
    " 'static__Milieu (vergunning)',\n",
    " 'agg__question_28',\n",
    " 'agg__monitoringResource_4634935',\n",
    " 'agg__Activity_08_AWB45_025',\n",
    " 'agg__question_Uitgebreid',\n",
    " 'agg__Activity_08_AWB45_170',\n",
    " 'agg__Activity_01_HOOFD_330',\n",
    " 'agg__org:resource_560530',\n",
    " 'agg__org:resource_4634935',\n",
    " 'agg__Activity_01_HOOFD_193',\n",
    " 'agg__question_other',\n",
    " 'agg__Activity_08_AWB45_060',\n",
    " 'agg__Activity_01_HOOFD_191',\n",
    " 'agg__Activity_08_AWB45_051_0',\n",
    " 'agg__timesincelastevent_sum',\n",
    " 'agg__Activity_01_HOOFD_490_1',\n",
    " 'agg__Activity_01_HOOFD_250',\n",
    " 'agg__Activity_08_AWB45_090_2',\n",
    " 'static__Kap',\n",
    " 'static__Milieu (neutraal wijziging)',\n",
    " 'agg__Activity_01_HOOFD_490_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50 columns (10% correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_PC10= ['agg__Activity_08_AWB45_020_2',\n",
    " 'static__Milieu (vergunning)',\n",
    " 'agg__question_28',\n",
    " 'agg__monitoringResource_4634935',\n",
    " 'agg__Activity_08_AWB45_025',\n",
    " 'agg__question_Uitgebreid',\n",
    " 'agg__Activity_08_AWB45_170',\n",
    " 'agg__Activity_01_HOOFD_330',\n",
    " 'agg__org:resource_560530',\n",
    " 'agg__org:resource_4634935',\n",
    " 'agg__Activity_01_HOOFD_193',\n",
    " 'agg__question_other',\n",
    " 'agg__Activity_08_AWB45_060',\n",
    " 'agg__Activity_01_HOOFD_191',\n",
    " 'agg__Activity_08_AWB45_051_0',\n",
    " 'agg__timesincelastevent_sum',\n",
    " 'agg__Activity_01_HOOFD_490_1',\n",
    " 'agg__Activity_01_HOOFD_250',\n",
    " 'agg__Activity_08_AWB45_090_2',\n",
    " 'static__Kap',\n",
    " 'static__Milieu (neutraal wijziging)',\n",
    " 'agg__Activity_08_AWB45_070_3',\n",
    " 'static__Gebiedsbescherming',\n",
    " 'agg__month_std',\n",
    " 'agg__timesincecasestart_std',\n",
    " 'agg__Activity_01_HOOFD_250_1',\n",
    " 'agg__question_42',\n",
    " 'agg__Activity_01_HOOFD_130',\n",
    " 'agg__Activity_01_HOOFD_250_2',\n",
    " 'agg__weekday_std',\n",
    " 'agg__Activity_08_OLO_100',\n",
    " 'agg__Activity_01_HOOFD_190_1',\n",
    " 'static__Responsible_actor_560530',\n",
    " 'agg__Activity_11_AH_II_010',\n",
    " 'agg__Activity_08_AWB45_050',\n",
    " 'agg__timesincecasestart_mean',\n",
    " 'agg__weekday_max',\n",
    " 'agg__timesincelastevent_max',\n",
    " 'agg__Activity_08_AWB45_090_1',\n",
    " 'agg__timesincelastevent_std',\n",
    " 'agg__Activity_08_AWB45_020_0',\n",
    " 'agg__timesincecasestart_sum',\n",
    " 'agg__open_cases_std',\n",
    " 'agg__timesincelastevent_mean',\n",
    " 'agg__Activity_01_HOOFD_190_2',\n",
    " 'static__Inrit/Uitweg',\n",
    " 'agg__Activity_01_HOOFD_470',\n",
    " 'agg__Activity_01_HOOFD_510_2',\n",
    " 'agg__month_min',\n",
    " 'agg__Activity_01_HOOFD_495']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### traffic_fines1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15% PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_PC15 = ['agg__Activity_Payment', 'agg__Activity_Send Fine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10% PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_PC10 = ['agg__Activity_Payment', 'agg__Activity_Send Fine', 'agg__Activity_Insert Date Appeal to Prefecture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bpic2017_accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20% PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_PC20= ['agg__CreditScore_std', 'agg__Selected_False']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15% PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_PC15= ['agg__CreditScore_std',\n",
    " 'agg__Selected_False',\n",
    " 'static__ApplicationType_New credit',\n",
    " 'static__ApplicationType_Limit raise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10% PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_PC10 = ['static__ApplicationType_Limit raise',\n",
    " 'static__ApplicationType_New credit',\n",
    " 'agg__Activity_A_Cancelled',\n",
    " 'agg__Activity_A_Submitted',\n",
    " 'agg__Activity_W_Validate application',\n",
    " 'agg__lifecycle:transition_ate_abort',\n",
    " 'agg__Accepted_False',\n",
    " 'agg__Accepted_True',\n",
    " 'agg__Selected_False',\n",
    " 'agg__CreditScore_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 5/5 [02:50<00:00, 34.16s/trial, best loss: -0.9229344126157142]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "import EncoderFactory\n",
    "from DatasetManager import DatasetManager\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import pickle\n",
    "\n",
    "import hyperopt\n",
    "from hyperopt import hp, Trials, fmin, tpe, STATUS_OK\n",
    "from hyperopt.pyll.base import scope\n",
    "from aix360.algorithms.rbm import LogisticRuleRegression\n",
    "from aix360.algorithms.rbm import FeatureBinarizer\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def create_and_evaluate_model(args): \n",
    "    global trial_nr\n",
    "    trial_nr += 1\n",
    "    \n",
    "    start = time.time()\n",
    "    score = 0\n",
    "    for cv_iter in range(n_splits):\n",
    "        \n",
    "        dt_test_prefixes = dt_prefixes[cv_iter]\n",
    "        dt_train_prefixes = pd.DataFrame()\n",
    "        for cv_train_iter in range(n_splits): \n",
    "            if cv_train_iter != cv_iter:\n",
    "                dt_train_prefixes = pd.concat([dt_train_prefixes, dt_prefixes[cv_train_iter]], axis=0)\n",
    "        \n",
    "        #remove training rows with negative values (should be a mistake)\n",
    "        #dt_train_prefixes = dt_train_prefixes[dt_train_prefixes.select_dtypes(include=[np.number]).ge(0).all(1)]\n",
    "\n",
    "        auc_totals = []\n",
    "        auc_best = 0\n",
    "\n",
    "        preds_all = []\n",
    "        test_y_all = []\n",
    "        test_y = dataset_manager.get_label_numeric(dt_test_prefixes)\n",
    "        train_y = dataset_manager.get_label_numeric(dt_train_prefixes)  \n",
    "        test_y_all.extend(test_y)       \n",
    "        #feature combiner\n",
    "        feature_combiner = FeatureUnion([(method, EncoderFactory.get_encoder(method, **cls_encoder_args)) for method in methods])\n",
    "        feature_combiner.fit(dt_train_prefixes, train_y)\n",
    "\n",
    "        #transform train dataset\n",
    "        dt_train_named= feature_combiner.transform(dt_train_prefixes)\n",
    "        dt_train_named = pd.DataFrame(dt_train_named)\n",
    "        names= feature_combiner.get_feature_names()\n",
    "        dt_train_named.columns = names\n",
    "        dt_train_named = dt_train_named[columns] \n",
    "        \n",
    "        #transform test dataset\n",
    "        dt_test_named = feature_combiner.transform(dt_test_prefixes)\n",
    "        dt_test_named = pd.DataFrame(dt_test_named)\n",
    "        names= feature_combiner.get_feature_names()\n",
    "        dt_test_named.columns = names\n",
    "        dt_test_named = dt_test_named[columns]\n",
    "        if cls_method =='xgboost':\n",
    "            cls = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                                n_estimators=500,\n",
    "                                                learning_rate= args['learning_rate'],\n",
    "                                                subsample=args['subsample'],\n",
    "                                                max_depth=int(args['max_depth']),\n",
    "                                                colsample_bytree=args['colsample_bytree'],\n",
    "                                                min_child_weight=int(args['min_child_weight']),\n",
    "                                                seed=random_state)\n",
    "\n",
    "            cls.fit(dt_train_named, train_y)\n",
    "            preds_pos_label_idx = np.where(cls.classes_ == 1)[0][0]\n",
    "            pred = cls.predict_proba(dt_test_named)[:,preds_pos_label_idx]\n",
    "            preds_all.extend(pred)\n",
    "            \n",
    "        elif cls_method =='glrm':\n",
    "            fb = FeatureBinarizer(negations=False, returnOrd=True, threshStr=False, numThresh=10 ) \n",
    "            dt_train_named, dfTrainStd = fb.fit_transform(dt_train_named)\n",
    "            dt_test_named, dfTestStd = fb.transform(dt_test_named)\n",
    "            cls = LogisticRuleRegression(lambda0=args['lambda0'], lambda1=args['lambda1'],\n",
    "                                         iterMax=10000)\n",
    "            # Train, print, and evaluate model\n",
    "            cls.fit(dt_train_named, pd.Series(train_y))\n",
    "            pred = cls.predict_proba(dt_test_named)\n",
    "            preds_all.extend(pred)\n",
    "        auc_total = roc_auc_score(test_y_all, preds_all)\n",
    "        score += roc_auc_score(test_y_all, preds_all)\n",
    "    \n",
    "    \n",
    "       \n",
    "        for k, v in args.items():\n",
    "            fout_all.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, k, v, score / n_splits))   \n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, \"processing_time\", time.time() - start, 0))   \n",
    "    fout_all.flush()\n",
    "    return {'loss': -score / n_splits, 'status': STATUS_OK, 'model': cls}\n",
    "\n",
    "dataset_ref = 'bpic2015_2_f2'\n",
    "params_dir = './params_dir'\n",
    "n_iter = 5\n",
    "cls_encoding = 'agg'\n",
    "cls_method = 'xgboost'\n",
    "\n",
    "column_selection = 'PC20'\n",
    "columns = columns_PC20\n",
    "\n",
    "method_name = \"%s_%s\"%(column_selection, cls_encoding)\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(1,6)],\n",
    "    \"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"],\n",
    "    \"bpic2015_half\": [\"bpic2015_%s_f2_half\"%(municipality) for municipality in range(1,6)]\n",
    "}\n",
    "\n",
    "encoding_dict = {\n",
    "    \"laststate\": [\"static\", \"last\"],\n",
    "    \"agg\": [\"static\", \"agg\"],\n",
    "    \"index\": [\"static\", \"index\"],\n",
    "    \"combined\": [\"static\", \"last\", \"agg\"],\n",
    "    \"onehot\": [\"static\",\"onehot\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "methods = encoding_dict[cls_encoding]\n",
    "    \n",
    "train_ratio = 0.8\n",
    "n_splits = 3\n",
    "random_state = 22\n",
    "\n",
    "# create results directory\n",
    "if not os.path.exists(os.path.join(params_dir)):\n",
    "    os.makedirs(os.path.join(params_dir))\n",
    "    \n",
    "for dataset_name in datasets:\n",
    "    \n",
    "    # read the data\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "    data = dataset_manager.read_dataset()\n",
    "    cls_encoder_args = {'case_id_col': dataset_manager.case_id_col, \n",
    "                        'static_cat_cols': dataset_manager.static_cat_cols,\n",
    "                        'static_num_cols': dataset_manager.static_num_cols, \n",
    "                        'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n",
    "                        'dynamic_num_cols': dataset_manager.dynamic_num_cols, \n",
    "                        'fillna': True}\n",
    "\n",
    "    # determine min and max (truncated) prefix lengths\n",
    "    min_prefix_length = 1\n",
    "    if \"traffic_fines\" in dataset_name:\n",
    "        max_prefix_length = 10\n",
    "    elif \"bpic2017\" in dataset_name:\n",
    "        max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "    else:\n",
    "        max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "\n",
    "    # split into training and test\n",
    "    train, _ = dataset_manager.split_data_strict(data, train_ratio, split=\"temporal\")\n",
    "    \n",
    "    \n",
    "    # prepare chunks for CV\n",
    "    dt_prefixes = []\n",
    "    class_ratios = []\n",
    "    for train_chunk, test_chunk in dataset_manager.get_stratified_split_generator(train, n_splits=n_splits):\n",
    "        class_ratios.append(dataset_manager.get_class_ratio(train_chunk))\n",
    "        # generate data where each prefix is a separate instance\n",
    "        dt_prefixes.append(dataset_manager.generate_prefix_data(test_chunk, min_prefix_length, max_prefix_length))\n",
    "    del train\n",
    "        \n",
    "    # set up search space\n",
    "    \n",
    "    if cls_method == \"xgboost\":\n",
    "        space = {'learning_rate': hp.uniform(\"learning_rate\", 0, 1),\n",
    "                 'subsample': hp.uniform(\"subsample\", 0.5, 1),\n",
    "                 'max_depth': scope.int(hp.quniform('max_depth', 4, 30, 1)),\n",
    "                 'colsample_bytree': hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "                 'min_child_weight': scope.int(hp.quniform('min_child_weight', 1, 6, 1))}\n",
    "      \n",
    "    if cls_method =='glrm':\n",
    "        space = {'lambda0': hp.uniform(\"lambda0\",0.00001, 0.01),\n",
    "                 'lambda1': hp.uniform(\"lambda1\", 0.00001,0.01)}\n",
    "    \n",
    "    \n",
    "    # optimize parameters\n",
    "    trial_nr = 0\n",
    "    trials = Trials()\n",
    "    fout_all = open(os.path.join(params_dir, \"param_optim_all_trials_%s_%s_%s.csv\" % (cls_method, dataset_name, method_name)), \"w\")\n",
    "    if \"prefix\" in method_name:\n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s;%s\\n\" % (\"iter\", \"dataset\", \"cls\", \"method\", \"nr_events\", \"param\", \"value\", \"score\"))   \n",
    "    else:\n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (\"iter\", \"dataset\", \"cls\", \"method\", \"param\", \"value\", \"score\"))   \n",
    "    rstate = np.random.RandomState(22)\n",
    "    best = fmin(create_and_evaluate_model, space, algo=tpe.suggest, max_evals=n_iter, trials=trials, rstate = rstate)\n",
    "    fout_all.close()\n",
    "\n",
    "    # write the best parameters\n",
    "    best_params = hyperopt.space_eval(space, best)\n",
    "    outfile = os.path.join(params_dir, \"optimal_params_%s_%s_%s.pickle\" % (cls_method, dataset_name, method_name))\n",
    "    # write to file\n",
    "    with open(outfile, \"wb\") as fout:\n",
    "        pickle.dump(best_params, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "164.988px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
